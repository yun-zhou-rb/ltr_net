{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RankNet as described in [1].\n",
    "[1] http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf\n",
    "\n",
    "[2] https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary case 1--> relevant 0--> no relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "input_dim = 10\n",
    "n_docs = 20\n",
    "n_rel2 = 5\n",
    "n_rel1 = 5\n",
    "n_irr = n_docs - n_rel2 - n_rel1\n",
    "y=np.array([2]*n_rel2+[1]*n_rel1+[0]*n_irr)\n",
    "unique, unique_index = np.unique(y, return_index=True)\n",
    "unique=np.flip(unique,axis=0).copy()\n",
    "unique = torch.from_numpy(unique)\n",
    "unique_index=np.flip(unique_index,axis=0).copy()\n",
    "unique_index = torch.from_numpy(unique_index)\n",
    "unique = unique.to(device)\n",
    "unique_index = unique_index.to(device)\n",
    "doc_features = np.random.randn(n_docs, input_dim)\n",
    "\n",
    "# Document scores.\n",
    "docs = torch.from_numpy(np.array(doc_features, dtype = \"float32\"))\n",
    "docs = docs.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  5,  5])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'diff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-19137421a68f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'diff'"
     ]
    }
   ],
   "source": [
    "torch.diff(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Model.\n",
    "L1=128//2\n",
    "L2=64//2\n",
    "L3=32//2\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(input_dim, L1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(L1, L2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(L2, L3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(L3, 1))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "doc_scores = model(docs)\n",
    "doc_scores[doc_scores<0]=0\n",
    "#print(doc_scores)\n",
    "ndcg_before=ndcg_at_k(doc_scores.tolist())\n",
    "print(ndcg_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : -125.90130615234375\n",
      "100 : -0.5084235668182373\n",
      "200 : -0.19798851013183594\n",
      "300 : -0.11553668975830078\n",
      "400 : -0.07860898971557617\n",
      "500 : -0.058447837829589844\n",
      "600 : -0.04600095748901367\n",
      "700 : -0.03762960433959961\n",
      "800 : -0.03165435791015625\n",
      "900 : -0.027197837829589844\n",
      "Finished Training\n",
      "tensor([[ -2.4126],\n",
      "        [ -0.6878],\n",
      "        [ -0.9928],\n",
      "        [ -0.9582],\n",
      "        [ -2.1191],\n",
      "        [ -9.0947],\n",
      "        [ -9.3157],\n",
      "        [ -9.1057],\n",
      "        [-10.7002],\n",
      "        [-15.1604],\n",
      "        [ -9.2678],\n",
      "        [-17.3323],\n",
      "        [-13.8892],\n",
      "        [-18.1695],\n",
      "        [ -9.6713],\n",
      "        [ -9.7246],\n",
      "        [-20.3776],\n",
      "        [-12.4374],\n",
      "        [ -8.3814],\n",
      "        [ -9.2085]], grad_fn=<ThAddmmBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<PutBackward>)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 1000\n",
    "loss = torch.zeros(1)\n",
    "loss = loss.to(device)\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    # forward\n",
    "    doc_scores = model(docs)\n",
    "    loss = torch.zeros(1)\n",
    "    for idx, ui in enumerate(unique_index[1:]):\n",
    "        o_ij=doc_scores[:n_rel2]-doc_scores[n_rel2:].view(-1)\n",
    "        c_ij = o_ij - torch.log(1.0+torch.exp(o_ij))\n",
    "        loss+=c_ij.sum()\n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model weights.\n",
    "    lr = 0.001\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param += lr * param.grad\n",
    "\n",
    "    # print statistics\n",
    "    if epoch % (n_epoch//10) ==0:\n",
    "        print(f\"{epoch} : {loss}\")\n",
    "          \n",
    "\n",
    "print('Finished Training')\n",
    "doc_scores = model(docs)\n",
    "print(doc_scores)\n",
    "doc_scores[doc_scores<0]=0\n",
    "print(doc_scores)\n",
    "ndcg_after=ndcg_at_k(doc_scores.tolist())\n",
    "print(ndcg_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.4749],\n",
       "        [ 9.6173],\n",
       "        [10.6030],\n",
       "        [ 8.4345],\n",
       "        [ 6.0750],\n",
       "        [ 2.2924],\n",
       "        [ 0.8137],\n",
       "        [ 1.8168],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 2.3252],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 2.6281],\n",
       "        [ 2.2651],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000]], grad_fn=<PutBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k=5, method=1):\n",
    "        \"\"\"Score is discounted cumulative gain (dcg)\n",
    "        Relevance is positive real values.  Can use binary\n",
    "        as the previous methods.\n",
    "        Example from\n",
    "        http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "        >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "        >>> dcg_at_k(r, 1)\n",
    "        3.0\n",
    "        >>> dcg_at_k(r, 1, method=1)\n",
    "        3.0\n",
    "        >>> dcg_at_k(r, 2)\n",
    "        5.0\n",
    "        >>> dcg_at_k(r, 2, method=1)\n",
    "        4.2618595071429155\n",
    "        >>> dcg_at_k(r, 10)\n",
    "        9.6051177391888114\n",
    "        >>> dcg_at_k(r, 11)\n",
    "        9.6051177391888114\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "            k: Number of results to consider\n",
    "            method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                    If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "        Returns:\n",
    "            Discounted cumulative gain\n",
    "        \"\"\"\n",
    "        r = np.asfarray(r)[:k]\n",
    "        if r.size > 0:\n",
    "            if method == 0:\n",
    "                return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "            elif method == 1:\n",
    "                return float(np.sum(r / np.log2(np.arange(2, r.size + 2))))\n",
    "            else:\n",
    "                raise ValueError('method must be 0 or 1.')\n",
    "        return 0.\n",
    "def ndcg_at_k(r, k=5, method=1):\n",
    "        \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "        Relevance is positive real values.  Can use binary\n",
    "        as the previous methods.\n",
    "        Example from\n",
    "        http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "        >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "        >>> ndcg_at_k(r, 1)\n",
    "        1.0\n",
    "        >>> r = [2, 1, 2, 0]\n",
    "        >>> ndcg_at_k(r, 4)\n",
    "        0.9203032077642922\n",
    "        >>> ndcg_at_k(r, 4, method=1)\n",
    "        0.96519546960144276\n",
    "        >>> ndcg_at_k([0], 1)\n",
    "        0.0\n",
    "        >>> ndcg_at_k([1], 2)\n",
    "        1.0\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "            k: Number of results to consider\n",
    "            method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                    If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "        Returns:\n",
    "            Normalized discounted cumulative gain\n",
    "        \"\"\"\n",
    "        dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "        if not dcg_max:\n",
    "            return 0.\n",
    "        return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
