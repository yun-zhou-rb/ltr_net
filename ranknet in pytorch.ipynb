{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RankNet as described in [1].\n",
    "[1] http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf\n",
    "\n",
    "[2] https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#binary case 1--> relevant 0--> no relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg(y,y_hat,k=10):\n",
    "    return ndcg_at_k(y[np.argsort(y_hat)[::-1]],k)\n",
    "def dcg_at_k(r, k=10):\n",
    "        \"\"\"Score is discounted cumulative gain (dcg)\n",
    "        Relevance is positive real values.  Can use binary\n",
    "        as the previous methods.\n",
    "        Example from\n",
    "        http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "        >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "        >>> dcg_at_k(r, 1)\n",
    "        3.0\n",
    "        >>> dcg_at_k(r, 1, method=1)\n",
    "        3.0\n",
    "        >>> dcg_at_k(r, 2)\n",
    "        5.0\n",
    "        >>> dcg_at_k(r, 2, method=1)\n",
    "        4.2618595071429155\n",
    "        >>> dcg_at_k(r, 10)\n",
    "        9.6051177391888114\n",
    "        >>> dcg_at_k(r, 11)\n",
    "        9.6051177391888114\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "            k: Number of results to consider\n",
    "            method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                    If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "        Returns:\n",
    "            Discounted cumulative gain\n",
    "        \"\"\"\n",
    "        r = np.asfarray(r)[:k]\n",
    "        if r.size > 0:\n",
    "            return float(np.sum((2**r-1) / np.log2(np.arange(2, r.size + 2))))\n",
    "        return 0.\n",
    "def ndcg_at_k(r, k=10, method=0):\n",
    "        \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "        Relevance is positive real values.  Can use binary\n",
    "        as the previous methods.\n",
    "        Example from\n",
    "        http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "        >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "        >>> ndcg_at_k(r, 1)\n",
    "        1.0\n",
    "        >>> r = [2, 1, 2, 0]\n",
    "        >>> ndcg_at_k(r, 4)\n",
    "        0.9203032077642922\n",
    "        >>> ndcg_at_k(r, 4, method=1)\n",
    "        0.96519546960144276\n",
    "        >>> ndcg_at_k([0], 1)\n",
    "        0.0\n",
    "        >>> ndcg_at_k([1], 2)\n",
    "        1.0\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "            k: Number of results to consider\n",
    "            method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                    If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "        Returns:\n",
    "            Normalized discounted cumulative gain\n",
    "        \"\"\"\n",
    "        dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "        if not dcg_max:\n",
    "            return 0.\n",
    "        return dcg_at_k(r, k) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "input_dim = 10\n",
    "n_docs = 20\n",
    "n_rel2 = 5\n",
    "n_rel1 = 5\n",
    "n_irr = n_docs - n_rel2 - n_rel1\n",
    "y=np.array([2]*n_rel2+[1]*n_rel1+[0]*n_irr)\n",
    "y_t=torch.from_numpy(y).float().to(device)\n",
    "unique, unique_index = np.unique(y, return_index=True)\n",
    "unique=np.flip(unique,axis=0).copy()\n",
    "unique = torch.from_numpy(unique)\n",
    "unique_index=np.flip(unique_index,axis=0).copy()\n",
    "unique_index = torch.from_numpy(unique_index)\n",
    "unique = unique.to(device)\n",
    "unique_index = unique_index.to(device)\n",
    "doc_features = np.random.randn(n_docs, input_dim)\n",
    "\n",
    "# Document scores.\n",
    "docs = torch.from_numpy(np.array(doc_features, dtype = \"float32\"))\n",
    "docs = docs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2635, 0.3412, 0.3176, 0.3422, 0.2773, 0.1904, 0.3029, 0.2635, 0.2415,\n",
      "        0.1859, 0.2241, 0.2341, 0.3001, 0.2208, 0.3259, 0.3043, 0.2335, 0.2753,\n",
      "        0.2420, 0.2644], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "0.7171530897897874\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Model.\n",
    "L1=128//2\n",
    "L2=64//2\n",
    "L3=32//2\n",
    "k=10\n",
    "rank_net = torch.nn.Sequential(\n",
    "    nn.Linear(input_dim, L1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(L1, L2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(L2, L3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(L3, 1))\n",
    "rank_lambda = copy.deepcopy(rank_net)\n",
    "rank_net = rank_net.to(device)\n",
    "rank_lambda = rank_lambda.to(device)\n",
    "doc_scores_init = rank_net(docs)\n",
    "print(doc_scores_init.view(-1))\n",
    "ndcg_before=compute_ndcg(y,doc_scores_init.view(-1).tolist(),k)\n",
    "print(ndcg_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : tensor([-118.8558], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "5 : tensor([-112.4036], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "10 : tensor([-102.3496], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "15 : tensor([-83.1775], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "20 : tensor([-55.5328], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "25 : tensor([-32.5745], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "30 : tensor([-19.8857], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "35 : tensor([-13.0919], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "40 : tensor([-9.1671], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "45 : tensor([-6.7600], device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "Finished Training\n",
      "tensor([ 6.5765,  6.1348,  5.3881,  5.9810,  4.1113,  3.0329,  2.4669,  2.9753,\n",
      "         2.5514,  2.9761, -1.0577, -1.0124,  0.2252, -0.5936, -1.0885, -0.5576,\n",
      "        -1.2160,  1.3547, -0.6664, -0.2327],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 50\n",
    "loss = torch.zeros(1)\n",
    "loss = loss.to(device)\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    # forward\n",
    "    doc_scores = rank_net(docs)\n",
    "    loss = torch.zeros(1).to(device)\n",
    "    for idx, ui in enumerate(unique_index[1:]):\n",
    "        o_ij=doc_scores[:ui]-doc_scores[ui:].view(-1)\n",
    "        c_ij=o_ij - torch.log(1.0+torch.exp(o_ij))\n",
    "        loss+=c_ij.sum()\n",
    "    # backward\n",
    "    rank_net.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model weights.\n",
    "    lr = 0.001\n",
    "    with torch.no_grad():\n",
    "        for param in rank_net.parameters():\n",
    "            param += lr * param.grad\n",
    "\n",
    "    ##print statistics\n",
    "    if epoch % (n_epoch//10) ==0:\n",
    "        print(f\"{epoch} : {loss}\")\n",
    "          \n",
    "\n",
    "print('Finished Training')\n",
    "doc_scores = rank_net(docs)\n",
    "print(doc_scores.view(-1))\n",
    "ndcg_after_net=compute_ndcg(y,doc_scores.view(-1).tolist(),k)\n",
    "print(ndcg_after_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2635, 0.3412, 0.3176, 0.3422, 0.2773, 0.1904, 0.3029, 0.2635, 0.2415,\n",
      "        0.1859, 0.2241, 0.2341, 0.3001, 0.2208, 0.3259, 0.3043, 0.2335, 0.2753,\n",
      "        0.2420, 0.2644], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "0.7171530897897874\n"
     ]
    }
   ],
   "source": [
    "ds=rank_lambda(docs)\n",
    "print(ds.view(-1))\n",
    "ndcg_before=compute_ndcg(y,ds.view(-1).tolist(),k)\n",
    "print(ndcg_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambdarank\n",
    "https://papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "tensor([13.2163, 10.5149,  8.9659,  8.8394,  6.9514,  4.7564,  3.5631,  3.6938,\n",
      "         3.1804,  3.5187, -0.9816, -0.7624,  0.9364, -0.4240, -1.2307, -0.8579,\n",
      "        -1.1998,  1.3178, -0.6483, -0.3315],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "N_max_dcg=dcg_at_k(y)\n",
    "loss = torch.zeros(1)\n",
    "loss = loss.to(device)\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    # forward\n",
    "    doc_scores = rank_lambda(docs)\n",
    "    loss = torch.zeros(1).to(device)\n",
    "    for idx, ui in enumerate(unique_index[1:]):\n",
    "        doc_scores = rank_lambda(docs)\n",
    "        o_ij=doc_scores[:ui]-doc_scores[ui:].view(-1)\n",
    "        first_part=1.0/(1.0+torch.exp(o_ij))\n",
    "        y_exp_t = 2**y_t[:,None]\n",
    "        second_part=y_exp_t[:ui]-y_exp_t[ui:].view(-1)\n",
    "        inv_log=1.0/torch.log2(torch.arange(2,len(doc_scores)+2).float())[:,None].to(device)\n",
    "        thrid_part=inv_log[:ui]-inv_log[ui:].view(-1)\n",
    "        whole_lambda=-N_max_dcg*first_part*second_part*thrid_part\n",
    "        loss+=whole_lambda.sum()\n",
    "    # backward\n",
    "    rank_lambda.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model weights.\n",
    "    lr = 0.001\n",
    "    with torch.no_grad():\n",
    "        for param in rank_lambda.parameters():\n",
    "            param += lr * param.grad\n",
    "\n",
    "    ##print statistics\n",
    "    #if epoch % (n_epoch//10) ==0:\n",
    "    #    print(f\"{epoch} : {loss}\")\n",
    "          \n",
    "\n",
    "print('Finished Training')\n",
    "doc_scores = rank_lambda(docs)\n",
    "print(doc_scores.view(-1))\n",
    "ndcg_after_lambda=compute_ndcg(y,doc_scores.view(-1).tolist(),k)\n",
    "print(ndcg_after_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_after_lambda-ndcg_after_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
